{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTV_ppK_qOyO"
   },
   "source": [
    "# <h1> ‚öôÔ∏è DiffAE Manipulation demo for Colab (simplified version) </h1>\n",
    "[![Full paper](https://img.shields.io/badge/Full_paper-arXiv-b31b1b.svg)](https://arxiv.org/abs/2111.15640)\n",
    "[![Web](https://img.shields.io/website-up-down-green-red/http/monip.org.svg)](https://diff-ae.github.io/)\n",
    "[![Github](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/phizaz/diffae)\n",
    "\n",
    "\n",
    "\n",
    "<small> this simplified notebook is particular created to demonstrate the manipulation process with DiffAE. <br>\n",
    "Note: DiffAE does not aim to be an image editing technique. However, with the capability to learn meaningful and rich representation with DiffAE, we show that DiffAE's representation can be meaningfully manipulated without using any fancy trick.\n",
    "\n",
    "For more detailed and more technical oriented version, please check out our github repository </small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XAIenn1GgrKa"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üîß Import libraries and tools </h3>\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from templates import *\n",
    "from templates_cls import *\n",
    "from experiment_classifier import ClsModel\n",
    "\n",
    "def show_images(images, cols = 1, titles = None, apply_convert=False):\n",
    "    if apply_convert: images = [convert(img) for img in images]\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, int(np.ceil(n_images/float(cols))), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images*2)\n",
    "    plt.show()\n",
    "\n",
    "def convert2rgb(img: torch.Tensor, adjust_scale=True):\n",
    "    if adjust_scale: img = (img + 1) / 2\n",
    "    return img.permute(1, 2, 0).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GomvNtQp0g1B"
   },
   "source": [
    "<h3> Model configuration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4oISkWa0g1C",
    "outputId": "2c3eec73-b848-4f1d-eed7-c68d701ee2d8"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> ‚öôÔ∏è Model Configuration</h3>\n",
    "#@markdown <small> we have provided 128 and 256 pretrained models; you may train our model with your own preference (checkout our github repository) .</small> <br>\n",
    "#@markdown <small> T_step:  is the number of step for generation in DiffAE. </small> <br>\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_resolution = 256 #@param [128, 256]\n",
    "T_inv = 200 #@param [50,100,125,200,250,500]\n",
    "T_step = 100 #@param [50,100,125,200,250,500]\n",
    "\n",
    "model_config = ffhq256_autoenc() if model_resolution == 256 else  ffhq128_autoenc_130M()\n",
    "\n",
    "print(model_config.name)\n",
    "\n",
    "\n",
    "# download model weight & its inferenced latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gukoW8gs0g1D",
    "outputId": "ae05f110-e28d-4bd9-f0f6-fc8b0888c55b"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> ‚ö° Model Initialization </h3>\n",
    "\n",
    "model = LitModel(model_config)\n",
    "state = torch.load(f'checkpoints/{model_config.name}/last.ckpt', map_location='cpu')\n",
    "model.load_state_dict(state['state_dict'], strict=False)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI6RpyvU0g1E"
   },
   "source": [
    "<h3> Classifier initialization </h3>\n",
    "We'll use a trained classifer weight to provide a class direction in our image manipulation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8GK1wMCMBAVE",
    "outputId": "99ac785d-cada-4f7f-cd24-53e22321946b"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3>  üê∂ üò∏  Classifier Initialization  </h3>\n",
    "\n",
    "classifier_config = ffhq256_autoenc_cls() if model_resolution == 256 else ffhq128_autoenc_cls()\n",
    "classifier_config.pretrain = None # a bit faster\n",
    "classifier = ClsModel(classifier_config)\n",
    "state = torch.load(f'checkpoints/{classifier_config.name}/last.ckpt', map_location='cpu')\n",
    "print('latent step:', state['global_step'])\n",
    "classifier.load_state_dict(state['state_dict'], strict=False)\n",
    "classifier.to(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTEiPEh-0g1F"
   },
   "source": [
    "<h2> Data preparation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTwjLWvIimjJ"
   },
   "source": [
    "Manipulate on uploaded img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "KEhFnfeBeYqK",
    "outputId": "eeb75b44-01c2-4039-a671-ad1ef64e2473"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üì§  Upload an image üë® üë©    for manipulation </h3>\n",
    "\n",
    "\"\"\"\n",
    "from google.colab import files\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import shutil\n",
    "\n",
    "\n",
    "uploaded_imgs_path = 'uploaded_imgs'\n",
    "if not osp.exists(uploaded_imgs_path) : os.makedirs(uploaded_imgs_path)\n",
    "\n",
    "def upload_img():\n",
    "  uploaded_img = files.upload()\n",
    "  uploadded_img_name = list(uploaded_img.keys())[0]\n",
    "\n",
    "  img = cv2.imread(uploadded_img_name)\n",
    "  cv2_imshow(img)\n",
    "\n",
    "  # shutil.move(uploadded_img_name, uploaded_imgs_path)\n",
    "  !mv \"{uploadded_img_name}\" \"{uploaded_imgs_path}\"\n",
    "\n",
    "  print(f\"move file {uploadded_img_name} to {uploaded_imgs_path} \")\n",
    "# print(uploaded_img.keys())\n",
    "# for fn in uploaded_img.keys():\n",
    "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#       name=fn, length=len(uploaded_img[fn])))\n",
    "\n",
    "upload_img()\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: We skipt the upload option as we already have the aligned image\n",
    "\"\"\"\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "img = cv2.imread(\"imgs/sandy.JPG\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "hF_moFS8k1gv",
    "outputId": "0cf5af95-d423-42ac-c8d4-ff989a61d94f"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üåÑ pre-process the uploaded image üë® üë© </h3>\n",
    "\n",
    "\"\"\"\n",
    "uploaded_imgs_path = 'uploaded_imgs'\n",
    "aligned_uploaded_imgs_path = 'aligned_uploaded_imgs'\n",
    "if not osp.exists(aligned_uploaded_imgs_path) : os.makedirs(aligned_uploaded_imgs_path)\n",
    "\n",
    "# !python align.py -i '{uploaded_imgs_path}' -o '{aligned_uploaded_imgs}'\n",
    "# os.system(f'python align.py -i {uploaded_imgs_path}/ -o {aligned_uploaded_imgs_path}/')\n",
    "\n",
    "\n",
    "!python align.py -i 'uploaded_imgs' -o 'aligned_uploaded_imgs'\n",
    "\"\"\"\n",
    "\n",
    "imgdataset_path = \"imgs_align\"  # path to your own image dataset\n",
    "data = ImageDataset(imgdataset_path, image_size=model_config.img_size, exts=['jpg','jpeg', 'JPG', 'png'], do_augment=False)\n",
    "batch = data[0][\"img\"]\n",
    "print(batch.shape) # C, H, W\n",
    "plt.imshow(convert2rgb(batch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add widgtes for target and amplitude\n",
    "from IPython.display import display \n",
    "from ipywidgets import Dropdown, IntSlider\n",
    "dd1 = Dropdown(options=['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'], \n",
    "               value='Wavy_Hair', description=\"Target:\")\n",
    "display(dd1) \n",
    "is1 = IntSlider(value=3, min=-5, max=5, step=1, description=\"Amplitude (e-1):\")\n",
    "display(is1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL2RGRUBVXJV"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3>  üë∂  üîÅ  üë® üîÅ  üë¥  Encode and Manipulate the uploaded image </h3>\n",
    "#@markdown <h5> manipulation amplitude: The Strongness of Manipulation <h5/>.\n",
    "# too strong would be resulted in artifact as it could dominate the original image information </small>\n",
    "\n",
    "# target_class = 'Young' #@param ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
    "target_class = dd1.value\n",
    "# manipulation_amplitude = 0.25 #@param {{type:\"slider\", min:-0.5, max:0.5, step:0.05}\n",
    "manipulation_amplitude = is1.value / 10\n",
    "\n",
    "# reverse the amplitude for some classes that are trained with opposite labels\n",
    "cls_manipulation_amplitude = manipulation_amplitude\n",
    "interpreted_target_class = target_class\n",
    "if target_class not in CelebAttrDataset.id_to_cls and f'No_{target_class}' in CelebAttrDataset.id_to_cls:\n",
    "  cls_manipulation_amplitude = -manipulation_amplitude\n",
    "  interpreted_target_class = f'No_{target_class}'\n",
    "\n",
    "\n",
    "batch = data[0]['img'][None]  # [None] adds batch dimension\n",
    "\n",
    "semantic_latent = model.encode(batch.to(device))\n",
    "stochastic_latent = model.encode_stochastic(batch.to(device), semantic_latent, T=T_inv)\n",
    "\n",
    "\n",
    "cls_id = CelebAttrDataset.cls_to_id[interpreted_target_class]\n",
    "class_direction = classifier.classifier.weight[cls_id]\n",
    "normalized_class_direction = F.normalize(class_direction[None, :], dim=1)\n",
    "\n",
    "normalized_semantic_latent = classifier.normalize(semantic_latent)\n",
    "normalized_manipulation_amp = cls_manipulation_amplitude * math.sqrt(512)\n",
    "normalized_manipulated_semantic_latent =  normalized_semantic_latent + normalized_manipulation_amp*normalized_class_direction\n",
    "\n",
    "manipulated_semantic_latent = classifier.denormalize(normalized_manipulated_semantic_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "GErniSL_Vbb6",
    "outputId": "d151a2eb-59b8-4c27-e2ee-2f9ef8c87328"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üòé Render Manipulated image </h3>\n",
    "\n",
    "manipulated_img = model.render(stochastic_latent, manipulated_semantic_latent, T=T_step)[0]\n",
    "original_img = data[0]['img']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(convert2rgb(original_img))\n",
    "ax[1].imshow(convert2rgb(manipulated_img, adjust_scale=False))\n",
    "plt.show()\n",
    "# plt.savefig('imgs_manipulated/compare.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "diffae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

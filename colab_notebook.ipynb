{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTV_ppK_qOyO"
   },
   "source": [
    "# <h1> ‚öôÔ∏è DiffAE Manipulation demo for Colab (simplified version) </h1>\n",
    "[![Full paper](https://img.shields.io/badge/Full_paper-arXiv-b31b1b.svg)](https://arxiv.org/abs/2111.15640)\n",
    "[![Web](https://img.shields.io/website-up-down-green-red/http/monip.org.svg)](https://diff-ae.github.io/)\n",
    "[![Github](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/phizaz/diffae)\n",
    "\n",
    "\n",
    "\n",
    "<small> this simplified notebook is particular created to demonstrate the manipulation process with DiffAE. <br>\n",
    "Note: DiffAE does not aim to be an image editing technique. However, with the capability to learn meaningful and rich representation with DiffAE, we show that DiffAE's representation can be meaningfully manipulated without using any fancy trick.\n",
    "\n",
    "For more detailed and more technical oriented version, please check out our github repository </small>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWHJ8eZ04GeR",
    "outputId": "ccaa9686-b163-4a1b-dee3-c9e98678b8b2"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üì• prepare and setup environment</h3>\n",
    "!pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2  pytorch-lightning==1.2.2  torchtext==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install scipy==1.5.4\n",
    "!pip install numpy==1.19.5\n",
    "!pip install tqdm\n",
    "!pip install pytorch-fid==0.2.0\n",
    "!pip install pandas==1.1.5\n",
    "!pip install lpips==0.1.4\n",
    "!pip install lmdb==1.2.1\n",
    "!pip install ftfy\n",
    "!pip install regex\n",
    "!pip install dlib requests\n",
    "\n",
    "\n",
    "!git clone https://github.com/phizaz/diffae\n",
    "!git pull https://github.com/phizaz/diffae\n",
    "\n",
    "\n",
    "\n",
    "%cd 'diffae'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "XAIenn1GgrKa"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üîß Import libraries and tools </h3>\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from templates import *\n",
    "from templates_cls import *\n",
    "from experiment_classifier import ClsModel\n",
    "\n",
    "def show_images(images, cols = 1, titles = None, apply_convert=False):\n",
    "    if apply_convert: images = [convert(img) for img in images]\n",
    "    assert((titles is None)or (len(images) == len(titles)))\n",
    "    n_images = len(images)\n",
    "    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n",
    "    fig = plt.figure()\n",
    "    for n, (image, title) in enumerate(zip(images, titles)):\n",
    "        a = fig.add_subplot(cols, int(np.ceil(n_images/float(cols))), n + 1)\n",
    "        if image.ndim == 2:\n",
    "            plt.gray()\n",
    "        plt.imshow(image)\n",
    "        a.set_title(title)\n",
    "    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images*2)\n",
    "    plt.show()\n",
    "\n",
    "def convert2rgb(img,adjust_scale=True):\n",
    "    convert_img = torch.tensor(img)\n",
    "    if adjust_scale: convert_img = (convert_img+1)/2\n",
    "    return (convert_img).permute(1, 2, 0).cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GomvNtQp0g1B"
   },
   "source": [
    "<h3> Model configuration </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w4oISkWa0g1C",
    "outputId": "2c3eec73-b848-4f1d-eed7-c68d701ee2d8"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> ‚öôÔ∏è Model Configuration</h3>\n",
    "#@markdown <small> we have provided 128 and 256 pretrained models; you may train our model with your own preference (checkout our github repository) .</small> <br>\n",
    "#@markdown <small> T_step:  is the number of step for generation in DiffAE. </small> <br>\n",
    "\n",
    "device = 'cuda:0'\n",
    "model_resolution = 256 #@param [128, 256]\n",
    "T_inv = 200 #@param [50,100,125,200,250,500]\n",
    "T_step = 100 #@param [50,100,125,200,250,500]\n",
    "\n",
    "model_config = ffhq256_autoenc() if model_resolution == 256 else  ffhq128_autoenc_130M()\n",
    "\n",
    "print(model_config.name)\n",
    "\n",
    "\n",
    "# download model weight & its inferenced latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJbqZm9Bc5LI",
    "outputId": "5ac6cfc0-fca9-451a-f3b9-1de70ba99a69"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üì•  Download pretrained weights </h3>\n",
    "\n",
    "\n",
    "\n",
    "download_paths = {\n",
    "    256: {\n",
    "        'diffae_weight': 'https://vistec-my.sharepoint.com/:u:/g/personal/nattanatc_pro_vistec_ac_th/ER56kn-uzlNAi39E1c784qQBkj5RI7-OAZC9QghQsIDQiQ?download=1' ,\n",
    "        'diffae_latent':  'https://vistec-my.sharepoint.com/:u:/g/personal/nattanatc_pro_vistec_ac_th/EcZqHAWy2v9GuaXuESDg1tUB4li-hf0aC-2SjeLxB3ASdQ?download=1' ,\n",
    "        'cls_weight': 'https://vistec-my.sharepoint.com/:u:/g/personal/nattanatc_pro_vistec_ac_th/ESzwEubKDNBEmYcbEr9O2jABtNqk_p7f2xYQ_t3PurXnHg?download=1'\n",
    "    },\n",
    "    128: {\n",
    "        'diffae_weight': 'https://vistec-my.sharepoint.com/:u:/g/personal/nattanatc_pro_vistec_ac_th/Eb36o-4NlSFGmpj3Vut2p-gBswf8PsUmeiqIEbA8XmAqWA?download=1',\n",
    "        'diffae_latent': 'https://vistec-my.sharepoint.com/:u:/g/personal/nattanatc_pro_vistec_ac_th/EQ9jjIjJRcJCs6h3d_LnJYoBvKk7zEMlHZn5btEKBEYoXg?download=1',\n",
    "        'cls_weight': 'https://vistec-my.sharepoint.com/:u:/g/personal/nattanatc_pro_vistec_ac_th/Eb36o-4NlSFGmpj3Vut2p-gBswf8PsUmeiqIEbA8XmAqWA?download=1'\n",
    "    }\n",
    "}\n",
    "\n",
    "weight_dir_path = f'checkpoints/{model_config.name}'\n",
    "if not osp.exists(weight_dir_path): os.makedirs(weight_dir_path)\n",
    "model_download_path = download_paths[model_resolution]['diffae_weight']\n",
    "latents_download_path = download_paths[model_resolution]['diffae_latent']\n",
    "!wget -O '{weight_dir_path}/last.ckpt' '{model_download_path}'\n",
    "!wget -O '{weight_dir_path}/latent.pkl' '{latents_download_path}'\n",
    "\n",
    "\n",
    "\n",
    "classifer_config = ffhq256_autoenc_cls() if model_resolution == 256 else ffhq128_autoenc_130M()\n",
    "weight_dir_path = f'checkpoints/{classifer_config.name}'\n",
    "if not osp.exists(weight_dir_path): os.makedirs(weight_dir_path)\n",
    "cls_download_path = download_paths[model_resolution]['cls_weight']\n",
    "!wget -O '{weight_dir_path}/last.ckpt' '{cls_download_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gukoW8gs0g1D",
    "outputId": "ae05f110-e28d-4bd9-f0f6-fc8b0888c55b"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> ‚ö° Model Initialization </h3>\n",
    "\n",
    "model = LitModel(model_config)\n",
    "state = torch.load(f'checkpoints/{model_config.name}/last.ckpt', map_location='cpu')\n",
    "model.load_state_dict(state['state_dict'], strict=False)\n",
    "model.ema_model.eval()\n",
    "model.ema_model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QI6RpyvU0g1E"
   },
   "source": [
    "<h3> Classifier initialization </h3>\n",
    "We'll use a trained classifer weight to provide a class direction in our image manipulation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8GK1wMCMBAVE",
    "outputId": "99ac785d-cada-4f7f-cd24-53e22321946b"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3>  üê∂ üò∏  Classifier Initialization  </h3>\n",
    "\n",
    "classifier_config = ffhq256_autoenc_cls() if model_resolution == 256 else ffhq128_autoenc_cls()\n",
    "classifier_config.pretrain = None # a bit faster\n",
    "classifier = ClsModel(classifier_config)\n",
    "state = torch.load(f'checkpoints/{classifer_config.name}/last.ckpt', map_location='cpu')\n",
    "print('latent step:', state['global_step'])\n",
    "classifier.load_state_dict(state['state_dict'], strict=False)\n",
    "classifier.to(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTEiPEh-0g1F"
   },
   "source": [
    "<h2> Data preparation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTwjLWvIimjJ"
   },
   "source": [
    "Manipulate on uploaded img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "KEhFnfeBeYqK",
    "outputId": "eeb75b44-01c2-4039-a671-ad1ef64e2473"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üì§  Upload an image üë® üë©    for manipulation </h3>\n",
    "\n",
    "\n",
    "from google.colab import files\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "import shutil\n",
    "\n",
    "\n",
    "uploaded_imgs_path = 'uploaded_imgs'\n",
    "if not osp.exists(uploaded_imgs_path) : os.makedirs(uploaded_imgs_path)\n",
    "\n",
    "def upload_img():\n",
    "  uploaded_img = files.upload()\n",
    "  uploadded_img_name = list(uploaded_img.keys())[0]\n",
    "\n",
    "  img = cv2.imread(uploadded_img_name)\n",
    "  cv2_imshow(img)\n",
    "\n",
    "  # shutil.move(uploadded_img_name, uploaded_imgs_path)\n",
    "  !mv \"{uploadded_img_name}\" \"{uploaded_imgs_path}\"\n",
    "\n",
    "  print(f\"move file {uploadded_img_name} to {uploaded_imgs_path} \")\n",
    "# print(uploaded_img.keys())\n",
    "# for fn in uploaded_img.keys():\n",
    "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#       name=fn, length=len(uploaded_img[fn])))\n",
    "\n",
    "upload_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "hF_moFS8k1gv",
    "outputId": "0cf5af95-d423-42ac-c8d4-ff989a61d94f"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üåÑ pre-process the uploaded image üë® üë© </h3>\n",
    "\n",
    "uploaded_imgs_path = 'uploaded_imgs'\n",
    "aligned_uploaded_imgs_path = 'aligned_uploaded_imgs'\n",
    "if not osp.exists(aligned_uploaded_imgs_path) : os.makedirs(aligned_uploaded_imgs_path)\n",
    "\n",
    "# !python align.py -i '{uploaded_imgs_path}' -o '{aligned_uploaded_imgs}'\n",
    "# os.system(f'python align.py -i {uploaded_imgs_path}/ -o {aligned_uploaded_imgs_path}/')\n",
    "\n",
    "\n",
    "!python align.py -i 'uploaded_imgs' -o 'aligned_uploaded_imgs'\n",
    "\n",
    "imgdataset_path = aligned_uploaded_imgs_path  # path to your own image dataset\n",
    "data = ImageDataset(imgdataset_path, image_size=model_config.img_size, exts=['jpg','jpeg', 'JPG', 'png'], do_augment=False)\n",
    "plt.imshow(convert2rgb(data[0]['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL2RGRUBVXJV"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3>  üë∂  üîÅ  üë® üîÅ  üë¥  Encode and Manipulate the uploaded image </h3>\n",
    "#@markdown <h5> manipulation amplitude: The Strongness of Manipulation <h5/>.\n",
    "# too strong would be resulted in artifact as it could dominate the original image information </small>\n",
    "target_class = 'Young' #@param ['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'Beard', 'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young']\n",
    "manipulation_amplitude = 0.25 #@param {{type:\"slider\", min:-0.5, max:0.5, step:0.05}\n",
    "\n",
    "# reverse the amplitude for some classes that are trained with opposite labels\n",
    "cls_manipulation_amplitude = manipulation_amplitude\n",
    "interpreted_target_class = target_class\n",
    "if target_class not in CelebAttrDataset.id_to_cls and f'No_{target_class}' in CelebAttrDataset.id_to_cls:\n",
    "  cls_manipulation_amplitude = -manipulation_amplitude\n",
    "  interpreted_target_class = f'No_{target_class}'\n",
    "\n",
    "\n",
    "\n",
    "batch = data[0]['img'][None]\n",
    "\n",
    "semantic_latent = model.encode(batch.to(device))\n",
    "stochastic_latent = model.encode_stochastic(batch.to(device), semantic_latent, T=T_inv)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cls_id = CelebAttrDataset.cls_to_id[interpreted_target_class]\n",
    "class_direction = classifier.classifier.weight[cls_id]\n",
    "normalized_class_direction = F.normalize(class_direction[None, :], dim=1)\n",
    "\n",
    "normalized_semantic_latent = classifier.normalize(semantic_latent)\n",
    "normalized_manipulation_amp = cls_manipulation_amplitude * math.sqrt(512)\n",
    "normalized_manipulated_semantic_latent =  normalized_semantic_latent + normalized_manipulation_amp*normalized_class_direction\n",
    "\n",
    "manipulated_semantic_latent = classifier.denormalize(normalized_manipulated_semantic_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "GErniSL_Vbb6",
    "outputId": "d151a2eb-59b8-4c27-e2ee-2f9ef8c87328"
   },
   "outputs": [],
   "source": [
    "#@markdown <h3> üòé Render Manipulated image </h3>\n",
    "\n",
    "\n",
    "\n",
    "manipulated_img = model.render(stochastic_latent, manipulated_semantic_latent, T=T_step)[0]\n",
    "original_img = data[0]['img']\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(convert2rgb(original_img))\n",
    "ax[1].imshow(convert2rgb(manipulated_img,adjust_scale=False))\n",
    "# plt.savefig('imgs_manipulated/compare.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2exxQAR36P1U"
   },
   "source": [
    "<h4> If you find our paper or code useful in your research, please consider citing: </h4>\n",
    "\n",
    "\n",
    "<pre><code> @inproceedings{preechakul2021diffusion,\n",
    "      title={Diffusion Autoencoders: Toward a Meaningful and Decodable Representation},\n",
    "      author={Preechakul, Konpat and Chatthee, Nattanat and Wizadwongsa, Suttisak and Suwajanakorn, Supasorn},\n",
    "      booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
    "      year={2022},\n",
    "}  <code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx9CO_yU0g1J"
   },
   "outputs": [],
   "source": [
    "# Note:\n",
    "\n",
    "# github is not published yet\n",
    "# # [![GitHub stars](https://img.shields.io/github/stars//phizaz/diffae.svg?style=social&label=Star&maxAge=2592000)](https://github.com/phizaz/diffae)\n",
    "\n",
    "# limit some class"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "interpreter": {
   "hash": "c34c6593d99c9985a1b30927262ec4c88246550b24b160c694e84311d56c55f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
